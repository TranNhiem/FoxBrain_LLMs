---
license: llama3.1
---

# FoxBrain æ¨¡å‹ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æª”æä¾›äº†é‹è¡Œ FoxBrain æ¨¡å‹çš„ç¤ºä¾‹ä»£ç¢¼ï¼Œè©²æ¨¡å‹å°ˆç‚ºæ•¸å­¸ç·¨ç¨‹å•é¡Œè§£æ±ºå’Œå¤šä»»å‹™è™•ç†è€Œè¨­è¨ˆã€‚æ­¤ç‰ˆæœ¬çš„ FoxBrain åŸºæ–¼ LLama 3.1 70B æ¨¡å‹é–‹ç™¼ã€‚

## ç›®éŒ„

- [å®‰è£æŒ‡å—](#å®‰è£æŒ‡å—)
- [æ¦‚è¿°](#æ¦‚è¿°)
- [è¼”åŠ©å‡½æ•¸](#è¼”åŠ©å‡½æ•¸)
- [ç³»çµ±æç¤ºè©](#ç³»çµ±æç¤ºè©)
- [å°è©±æ¨¡æ¿](#å°è©±æ¨¡æ¿)
- [éƒ¨ç½²æ–¹æ³•](#éƒ¨ç½²æ–¹æ³•)
  - [1. ç›´æ¥ä½¿ç”¨ Python APIï¼ˆæœ¬åœ°å¯¦ç¾ï¼‰](#1-ç›´æ¥ä½¿ç”¨-python-apiæœ¬åœ°å¯¦ç¾)
  - [2. OpenAI å…¼å®¹ API ä¼ºæœå™¨](#2-openai-å…¼å®¹-api-ä¼ºæœå™¨)
  - [3. é€²éšï¼šä½¿ç”¨ VLLM é€²è¡Œå‡½æ•¸èª¿ç”¨](#3-é€²éšä½¿ç”¨-vllm-é€²è¡Œå‡½æ•¸èª¿ç”¨)
- [æ¸¬è©¦éçš„ FoxBrain éƒ¨ç½²ç¯„ä¾‹](#æ¸¬è©¦éçš„-foxbrain-éƒ¨ç½²ç¯„ä¾‹)
- [è§£æ FoxBrain çµæ§‹åŒ–è¼¸å‡º](#è§£æ-foxbrain-çµæ§‹åŒ–è¼¸å‡º)
- [æ€§èƒ½å„ªåŒ–](#æ€§èƒ½å„ªåŒ–)
- [å¸¸è¦‹å•é¡Œæ’è§£](#å¸¸è¦‹å•é¡Œæ’è§£)
- [GPU å’Œ BF16 ç²¾åº¦](#gpu-å’Œ-bf16-ç²¾åº¦)

## å®‰è£æŒ‡å—

ç¢ºä¿æ‚¨å·²å®‰è£ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚ç„¶å¾Œï¼Œä½¿ç”¨ pip å®‰è£æ‰€éœ€çš„ä¾è³´é …ã€‚æ‚¨å¯ä»¥å–®ç¨å®‰è£è»Ÿä»¶åŒ…æˆ–ä½¿ç”¨ requirements æ–‡ä»¶ã€‚

### ä½¿ç”¨ pip
```bash
pip install vllm==0.6.6.post1
pip install transformers==4.48.0 
```

## è¼”åŠ©å‡½æ•¸

ä»¥ä¸‹æ˜¯ç”¨æ–¼è§£ææ¨¡å‹ç”Ÿæˆå›æ‡‰çš„è¼”åŠ©å‡½æ•¸ï¼š
```python
import re

def check_patterns(response):
    """
    æª¢æŸ¥å›æ‡‰æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„ XML æ¨™ç±¤ã€‚
    
    åƒæ•¸ï¼š
        response (str)ï¼šæ¨¡å‹ç”Ÿæˆçš„å›æ‡‰
    
    è¿”å›ï¼š
        strï¼šè§£æå¾Œçš„å›æ‡‰ï¼Œå¦‚æœæ¨¡å¼ä¸å®Œæ•´å‰‡è¿”å› 'Missing'
    """
    patterns = {
        'answer': r'<answer>(.*?)</answer>',
        'reflection': r'<reflection>(.*?)</reflection>',
        'steps': r'<step>(.*?)</step>',
        'count': r'<count>(.*?)</count>'
    }
    
    matches = {
        'answer': re.search(patterns['answer'], response, re.DOTALL),
        'reflection': re.search(patterns['reflection'], response, re.DOTALL),
        'steps': re.findall(patterns['steps'], response, re.DOTALL),
        'count': re.findall(patterns['count'], response, re.DOTALL)
    }
    
    return "Missing" if not all([matches['answer'], matches['reflection'], matches['steps'], matches['count']]) else response

def parse_response(response):
    """
    è§£ææ¨¡å‹çš„å›æ‡‰ä¸¦æå–é—œéµçµ„ä»¶ã€‚
    
    åƒæ•¸ï¼š
        response (str)ï¼šæ¨¡å‹ç”Ÿæˆçš„å›æ‡‰
    
    è¿”å›ï¼š
        tupleï¼šè§£æå¾Œçš„ç­”æ¡ˆã€åæ€ã€æ­¥é©Ÿå’Œèªªæ˜
    """
    response_check = check_patterns(response)
    
    if response_check == "Missing":
        clarification_match = re.search(r'<clarification>(.*?)</clarification>', response, re.DOTALL)
        clarification = clarification_match.group(1).strip() if clarification_match else response
        return "", "", [], clarification
    else:
        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
        reflection_match = re.search(r'<reflection>(.*?)</reflection>', response, re.DOTALL)
        
        answer = answer_match.group(1).strip() if answer_match else ""
        reflection = reflection_match.group(1).strip() if reflection_match else ""
        steps = re.findall(r'<step>(.*?)</step>', response, re.DOTALL)
        
        return answer, reflection, steps, ""
```

## ç³»çµ±æç¤ºè©

FoxBrain ä½¿ç”¨çš„é»˜èªç³»çµ±æç¤ºè©ï¼ˆå¯æ ¹æ“šéœ€è¦è‡ªå®šç¾©ï¼‰ï¼š

```DEFAULT_SYSTEM_PROMPT = """You are a FoxBrain AI Assistant created and Developed by Foxconn (é´»æµ·ç ”ç©¶é™¢). When given a human question related to multiple choices, as an expert & helpful reasoning assistant, your task is to provide a detailed answer following the instructions template below:

**Instructions:**

1. **Determine Budget**: Based on the question's complexity, decide on an appropriate step budget as an integer between 1 and 9 (e.g., 1 to 3 for an easy question, 3 to 6 for a medium question, 6 to 9 for a very difficult question). Reset the counter between `<count>` and `</count>` to this budget.

2. **Step-by-Step Solution**: Provide a logical, step-by-step solution, including code snippets where appropriate.
   - Enclose each step within `<step>` and `</step>` tags.
   - After each step, decrement the budget within `<count>` and `</count>` tags.
   - Stop when the budget reaches 0; you don't have to use all steps.

3. **Self-Reflection**: If unsure how to proceed based on self-reflection and reward, decide if you need to revise previous steps.
   - Enclose self-reflections within `<reflection>` and `</reflection>` tags.
   - Enclose the quality score within `<reward>` and `</reward>` tags.

4. **Final Answer**: After completing the reasoning steps, synthesize the steps into the final comprehensive detailed answer within `<answer>` and `</answer>` tags.

5. **Evaluation**: Provide a critical, honest, and subjective self-evaluation of your reasoning process within `<reflection>` and `</reflection>` tags.

6. **Quality Score**: Assign a quality score as a float between 0.0 and 1.0 within `<reward>` and `</reward>` tags.

**Example Format:**
<count> [starting budget] </count>
<step> [Step 1 with code snippets if necessary] </step>
<count> [remaining budget] </count>
<step> [Step 2 with code snippets if necessary] </step>
<reflection> [Evaluation of steps so far] </reflection>
<reward> [Quality score] </reward>
<count> [remaining budget] </count>
...
<answer>
[Final comprehensive answer]
</answer>
<reflection> [Final evaluation] </reflection>
<reward> [Quality score] </reward>"""

# ç³»çµ±æç¤ºè©å¯ä»¥é¸æ“‡æ€§ä½¿ç”¨æˆ–ç•™ç©º
# DEFAULT_SYSTEM_PROMPT = ""
```

## å°è©±æ¨¡æ¿

FoxBrain æ¨¡å‹éœ€è¦ä½¿ç”¨ Llama 3.1 å…¼å®¹çš„å°è©±æ¨¡æ¿ä¾†æ­£ç¢ºæ ¼å¼åŒ–æ¶ˆæ¯ã€‚æ¨¡æ¿æ–‡ä»¶ `llama31_chattemplate.jinja` å°æ–¼æ­£ç¢ºæ“ä½œè‡³é—œé‡è¦ï¼š

1. å¾ä»£ç¢¼åº«ä¸‹è¼‰å°è©±æ¨¡æ¿æ–‡ä»¶
2. å°‡å…¶ä¿å­˜åˆ°ç³»çµ±ä¸­å¯è¨ªå•çš„ä½ç½®
3. å•Ÿå‹• VLLM ä¼ºæœå™¨æ™‚æŒ‡å®šå…¶è·¯å¾‘ï¼š

```bash
--chat-template "llama31_chattemplate.jinja çš„è·¯å¾‘"
```

å°è©±æ¨¡æ¿å°ä»¥ä¸‹æ–¹é¢è‡³é—œé‡è¦ï¼š
- æ­£ç¢ºæ ¼å¼åŒ–æ¨¡å‹çš„å°è©±æ­·å²
- ç¢ºä¿ä¸€è‡´è™•ç†ç³»çµ±æç¤ºè©ã€ç”¨æˆ¶æ¶ˆæ¯å’ŒåŠ©æ‰‹å›æ‡‰
- æ”¯æŒå‡½æ•¸/å·¥å…·èª¿ç”¨çš„é æœŸæ¶ˆæ¯æ ¼å¼

ç•¶ä½¿ç”¨ç›´æ¥ Python API æ–¹æ³•æ™‚ï¼Œæ¨¡æ¿æœƒé€šéåˆ†è©å™¨è‡ªå‹•æ‡‰ç”¨ï¼š

```python
# ç¢ºä¿ä½¿ç”¨ç›¸åŒçš„æ¨¡æ¿åˆå§‹åŒ–åˆ†è©å™¨
tokenizer = AutoTokenizer.from_pretrained(model_id)
# å¦‚æœéœ€è¦ï¼ŒæŒ‡å®šå°è©±æ¨¡æ¿è·¯å¾‘
# tokenizer.chat_template = open("llama31_chattemplate.jinja çš„è·¯å¾‘").read()

formatted_prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
```

## åŸºæœ¬é…ç½®

```python 
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_id="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘"
number_gpus = 2 ## å°æ–¼ BF16 éœ€è¦ 1 GPUï¼Œå°æ–¼ FP8 éœ€è¦ A100 æˆ– H100 80GB Vram
# æ¡æ¨£åƒæ•¸
tokenizer = AutoTokenizer.from_pretrained(model_id_tokenizer)
sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=4096, 
            # repetition_penalty=1.5,
            stop=['<|eot_id|>', '<|end_of_text|>'], 
            #stop_token_ids=[128001,128009]
        )

# åˆå§‹åŒ– VLLM æ¨¡å‹
llm = LLM(
    model=model_id,
    tensor_parallel_size=number_gpus,  # ä½¿ç”¨çš„ GPU æ•¸é‡
    dtype="bfloat16",  # ä½¿ç”¨ bfloat16 æé«˜ Llama æ¨¡å‹æ€§èƒ½
)

# é‹è¡Œæ¨ç†çš„å‡½æ•¸
def run_inference(prompt, system_prompt=DEFAULT_SYSTEM_PROMPT):
    # ä»¥èŠå¤©æ ¼å¼æ ¼å¼åŒ– Llama æ¨¡å‹çš„è¼¸å…¥
    messages = []
    
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": prompt})
    
    # æ‡‰ç”¨å°è©±æ¨¡æ¿æ ¼å¼åŒ–å°è©±
    formatted_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # é‹è¡Œæ¨ç†
    outputs = llm.generate(formatted_prompt, sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    return generated_text

# ä½¿ç”¨ç¯„ä¾‹
question = "å¦‚æœä¸€åˆ—ç«è»Šä»¥ 120 å…¬é‡Œ/å°æ™‚çš„é€Ÿåº¦è¡Œé§›ï¼Œå¦ä¸€åˆ—ç«è»Šä»¥ 80 å…¬é‡Œ/å°æ™‚çš„é€Ÿåº¦æœç›¸åæ–¹å‘è¡Œé§›ï¼Œå¦‚æœå®ƒå€‘å¾åŒä¸€è»Šç«™å‡ºç™¼ï¼Œéœ€è¦å¤šé•·æ™‚é–“æ‰èƒ½ç›¸è· 500 å…¬é‡Œï¼Ÿ"
response = run_inference(question)

# è§£æå›æ‡‰ä»¥æå–çµæ§‹åŒ–ä¿¡æ¯
answer, reflection, steps, clarification = parse_response(response)

print(f"ç­”æ¡ˆ: {answer}")
print(f"æ­¥é©Ÿ: {steps}")
print(f"åæ€: {reflection}")
```

---

# VLLM æ¨ç†å¯¦ç¾

VLLM æä¾›ä¸‰ç¨®ä¸»è¦éƒ¨ç½²æ–¹æ³•ä¾†é‹è¡Œæ‚¨çš„ FoxBrain æ¨¡å‹ã€‚æ¯ç¨®æ–¹æ³•éƒ½æœ‰ä¸åŒçš„å„ªå‹¢ï¼Œå–æ±ºæ–¼æ‚¨çš„ä½¿ç”¨å ´æ™¯ï¼š

## 1. ç›´æ¥ä½¿ç”¨ Python APIï¼ˆæœ¬åœ°å¯¦ç¾ï¼‰

æœ€ç°¡å–®çš„æ–¹æ³•æ˜¯ç›´æ¥åœ¨æ‡‰ç”¨ç¨‹åºä¸­ä½¿ç”¨ VLLM çš„ Python APIã€‚æ­¤æ–¹æ³•æä¾›æœ€å¤šæ§åˆ¶æ¬Šï¼Œé©åˆï¼š
- è‡ªå®šç¾© Python æ‡‰ç”¨ç¨‹åº
- å¾Œç«¯æœå‹™
- æ‰¹è™•ç†
- ç ”ç©¶å’Œå¯¦é©—

ä¸Šé¢çš„ä»£ç¢¼ç¤ºä¾‹æ¼”ç¤ºäº†é€™ç¨®æ–¹æ³•ã€‚ä¸»è¦çµ„ä»¶åŒ…æ‹¬ï¼š

- **æ¨¡å‹åˆå§‹åŒ–**ï¼šä½¿ç”¨ç‰¹å®šç¡¬ä»¶é…ç½®åŠ è¼‰æ¨¡å‹
- **è¼¸å…¥æ ¼å¼åŒ–**ï¼šä½¿ç”¨å°è©±æ¨¡æ¿æ­£ç¢ºæ ¼å¼åŒ–è¼¸å…¥
- **æ¨ç†**ï¼šä½¿ç”¨æŒ‡å®šåƒæ•¸ç”Ÿæˆå®Œæˆ
- **å›æ‡‰è§£æ**ï¼šå¾æ¨¡å‹è¼¸å‡ºä¸­æå–çµæ§‹åŒ–ä¿¡æ¯

### ä»£ç¢¼ç¤ºä¾‹è©³è§£

```python
# ä½¿ç”¨ç¡¬ä»¶è¨­ç½®åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="FoxBrain_æ¨¡å‹çš„è·¯å¾‘",
    tensor_parallel_size=2,  # åˆ†ä½ˆåœ¨ 2 å€‹ GPU ä¸Š
    dtype="bfloat16",        # ä½¿ç”¨ BF16 ç²¾åº¦
)

# ä½¿ç”¨å°è©±æ¨¡æ¿æ ¼å¼åŒ–è¼¸å…¥
formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# é‹è¡Œæ¨ç†
outputs = llm.generate(formatted_prompt, sampling_params)
generated_text = outputs[0].outputs[0].text
```

---

## 2. OpenAI å…¼å®¹ API ä¼ºæœå™¨

å°æ–¼å·²ç¶“ä½¿ç”¨ OpenAI API æ§‹å»ºçš„æ‡‰ç”¨ç¨‹åºæˆ–éœ€è¦æ¨™æº–åŒ– REST æ¥å£æ™‚ï¼ŒVLLM æä¾›äº† OpenAI å…¼å®¹ä¼ºæœå™¨ã€‚æ­¤æ–¹æ³•é©åˆï¼š
- Web æ‡‰ç”¨ç¨‹åº
- è·¨èªè¨€æ‡‰ç”¨ç¨‹åº
- èˆ‡ç¾æœ‰ OpenAI åŸºç¤å·¥å…·é›†æˆ
- éœ€è¦ REST API çš„æœå‹™

### ä¼ºæœå™¨è¨­ç½®

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å•Ÿå‹• VLLM OpenAI å…¼å®¹ä¼ºæœå™¨ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
    --model "FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘" \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --dtype bfloat16 \
    --chat-template "llama31_chattemplate.jinja çš„è·¯å¾‘"
```

> **æ³¨æ„**ï¼š`--host 0.0.0.0` åƒæ•¸ä½¿ä¼ºæœå™¨å¯å¾ä»»ä½• IP åœ°å€è¨ªå•ã€‚åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­å‡ºæ–¼å®‰å…¨è€ƒæ…®ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ç¶å®šåˆ°ç‰¹å®š IP åœ°å€ã€‚`--port` åƒæ•¸ï¼ˆæœ¬ä¾‹ä¸­ç‚º 8000ï¼‰ç¢ºå®šä¼ºæœå™¨ç›£è½çš„ç«¯å£ã€‚

### å®¢æˆ¶ç«¯ä½¿ç”¨

ä¼ºæœå™¨é‹è¡Œå¾Œï¼Œä½¿ç”¨ OpenAI å®¢æˆ¶ç«¯èˆ‡å…¶äº¤äº’ï¼š

```python
from openai import OpenAI

# é…ç½®å®¢æˆ¶ç«¯ä½¿ç”¨æ‚¨çš„ VLLM ä¼ºæœå™¨
client = OpenAI(
    base_url="http://localhost:8000/v1",  # åŒ¹é…æ‚¨ä¼ºæœå™¨çš„ IP/ä¸»æ©Ÿåå’Œç«¯å£
    api_key="dummy-key"  # å°æ–¼æœ¬åœ° VLLM ä¼ºæœå™¨ï¼ŒAPI å¯†é‘°ä¸é‡è¦
)

# ç™¼é€è«‹æ±‚
response = client.chat.completions.create(
    model="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "æ³•åœ‹çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ"}
    ],
    temperature=0.7,
    max_tokens=2048,
)

# è™•ç†å›æ‡‰
answer = response.choices[0].message.content
```

> **é‡è¦**ï¼šå®¢æˆ¶ç«¯é…ç½®ä¸­çš„ `base_url` å¿…é ˆèˆ‡ä¼ºæœå™¨çš„ IP åœ°å€å’Œç«¯å£åŒ¹é…ã€‚å¦‚æœæ‚¨çš„ä¼ºæœå™¨åœ¨ IP ç‚º 192.168.1.100ã€ç«¯å£ç‚º 8000 çš„é ç¨‹æ©Ÿå™¨ä¸Šé‹è¡Œï¼Œæ‚¨çš„ base_url æ‡‰ç‚º `http://192.168.1.100:8000/v1`ã€‚å°æ–¼æœ¬åœ°æ¸¬è©¦ï¼Œä½¿ç”¨ `localhost` æˆ– `127.0.0.1`ã€‚

---

## 3. é€²éšï¼šä½¿ç”¨ VLLM é€²è¡Œå‡½æ•¸èª¿ç”¨

VLLM æ”¯æŒé¡ä¼¼æ–¼ OpenAI å¯¦ç¾çš„å‡½æ•¸/å·¥å…·èª¿ç”¨åŠŸèƒ½ã€‚é€™å…è¨±æ‚¨çš„ FoxBrain æ¨¡å‹ï¼š
- èª¿ç”¨å¤–éƒ¨å·¥å…·å’Œ API
- è§£æ±ºéœ€è¦å¤–éƒ¨æ•¸æ“šçš„å•é¡Œ
- åŸ·è¡Œè¨ˆç®—æˆ–æ•¸æ“šæŸ¥è©¢
- è™•ç†é€™äº›èª¿ç”¨çš„çµæœ

é€™ç¨®æ–¹æ³•åœ¨ OpenAI å…¼å®¹ API ä¼ºæœå™¨åŸºç¤ä¸Šé€²è¡Œäº†é¡å¤–é…ç½®ã€‚

### ä½¿ç”¨å‡½æ•¸èª¿ç”¨çš„ä¼ºæœå™¨è¨­ç½®

é€šéæ·»åŠ ç‰¹å®šæ¨™èªŒå•Ÿç”¨å‡½æ•¸èª¿ç”¨ä¾†å•Ÿå‹•ä¼ºæœå™¨ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
    --model "FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘" \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json \
    --chat-template "llama31_chattemplate.jinja çš„è·¯å¾‘"
```

> **æ³¨æ„**ï¼šåœ¨è·¨ä¸åŒæ©Ÿå™¨éƒ¨ç½²æ™‚ï¼Œå¦‚æœè¦é™åˆ¶è¨ªå•ï¼Œè«‹å°‡ `--host 0.0.0.0` æ›¿æ›ç‚ºä¼ºæœå™¨çš„ IP åœ°å€ã€‚ç„¶å¾Œæ›´æ–°æ‰€æœ‰å®¢æˆ¶ç«¯çš„ `base_url` å€¼ä»¥æŒ‡å‘æ­¤ IP åœ°å€ï¼ˆä¾‹å¦‚ï¼Œ`http://server-ip-address:8000/v1`ï¼‰ã€‚

### æ¸¬è©¦éçš„ FoxBrain éƒ¨ç½²ç¯„ä¾‹

ä»¥ä¸‹å‘½ä»¤å·²ç¶“éæ¸¬è©¦ä¸¦é©—è­‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°é‹è¡Œå…·æœ‰å‡½æ•¸èª¿ç”¨åŠŸèƒ½çš„ FoxBrain æ¨¡å‹ï¼š

```bash
vllm serve "FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘" \
    --dtype auto \
    --api-key "æ‚¨çš„ API å¯†é‘°" \
    --port 8883 \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json \
    --chat-template "llama31_chattemplate.jinja çš„è·¯å¾‘" \
    --max-model-len 32768 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.97
```

æ­¤å‘½ä»¤åŒ…æ‹¬ï¼š
- è‡ªå‹• dtype æª¢æ¸¬ä»¥ç²å¾—æœ€ä½³æ€§èƒ½
- åœ¨ç«¯å£ 8883 ä¸Šå…¬é–‹ APIï¼Œå¸¶æœ‰è‡ªå®šç¾© API å¯†é‘°
- å•Ÿç”¨å‡½æ•¸èª¿ç”¨ä¸¦ä½¿ç”¨ Llama3 JSON è§£æå™¨
- è¨­ç½®è¼ƒå¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆ32K æ¨™è¨˜ï¼‰
- åœ¨ 2 å€‹ GPU ä¸Šåˆ†ä½ˆæ¨¡å‹
- æœ€å¤§åŒ– GPU å…§å­˜ä½¿ç”¨ç‡ï¼ˆ97%ï¼‰

ç¤ºä¾‹ Python è…³æœ¬ `foxbrain_function_calling_example.py` æ¼”ç¤ºäº†å¦‚ä½•é€£æ¥åˆ°æ­¤ VLLM ä¼ºæœå™¨ä¸¦ä½¿ç”¨ FoxBrain çš„å‡½æ•¸èª¿ç”¨åŠŸèƒ½ã€‚è©²è…³æœ¬åŒ…æ‹¬å¤©æ°£æŸ¥è©¢å’Œè¨ˆç®—å™¨å‡½æ•¸çš„å¯¦ç¾ï¼Œä»¥åŠç”¨æ–¼è™•ç†å‡½æ•¸èª¿ç”¨å›æ‡‰çš„å¼·å¤§ JSON è§£æåŠŸèƒ½ã€‚

> **é‡è¦**ï¼š`llama31_chattemplate.jinja` æ–‡ä»¶å°æ–¼ FoxBrain æ¨¡å‹çš„æ­£ç¢ºå‡½æ•¸èª¿ç”¨è‡³é—œé‡è¦ã€‚ç¢ºä¿å¾ä»£ç¢¼åº«ä¸‹è¼‰æ­¤æ¨¡æ¿æ–‡ä»¶ä¸¦åœ¨ VLLM å‘½ä»¤ä¸­æŒ‡å®šå…¶æ­£ç¢ºè·¯å¾‘ã€‚å¦‚æœæ²’æœ‰æ­£ç¢ºçš„å°è©±æ¨¡æ¿ï¼Œæ¨¡å‹å¯èƒ½ç„¡æ³•æ­£ç¢ºç†è§£æˆ–ç”Ÿæˆå‡½æ•¸èª¿ç”¨ã€‚

ä»¥ä¸‹æ˜¯æ­¤ç¤ºä¾‹ä¸­å®¢æˆ¶ç«¯ä»£ç¢¼çš„é—œéµéƒ¨åˆ†ï¼š

```python
from openai import OpenAI
import json
import re

# é…ç½®å®¢æˆ¶ç«¯ä½¿ç”¨æ‚¨çš„ VLLM ä¼ºæœå™¨
client = OpenAI(
    base_url="http://127.0.0.1:8883/v1",  # åŒ¹é…æ‚¨ä¼ºæœå™¨çš„ IP/ä¸»æ©Ÿåå’Œç«¯å£
    api_key="æ‚¨çš„ API å¯†é‘°"  # åŒ¹é…æ‚¨çš„ API å¯†é‘°
)

> **é‡è¦**ï¼šè·¨æ©Ÿå™¨éƒ¨ç½²æ™‚ï¼š
> - åœ¨åŒä¸€å°æ©Ÿå™¨ä¸Šé€²è¡Œæœ¬åœ°æ¸¬è©¦ï¼šä½¿ç”¨ `base_url="http://127.0.0.1:PORT/v1"` æˆ– `base_url="http://localhost:PORT/v1"`
> - å¾å¦ä¸€å°æ©Ÿå™¨é€£æ¥ï¼šä½¿ç”¨ `base_url="http://SERVER_IP:PORT/v1"`ï¼Œå…¶ä¸­ SERVER_IP æ˜¯é‹è¡Œ VLLM çš„ä¼ºæœå™¨ IP åœ°å€
> - å§‹çµ‚ç¢ºä¿ç«¯å£è™Ÿèˆ‡ VLLM ä¼ºæœå™¨å‘½ä»¤ä¸­æŒ‡å®šçš„ç«¯å£è™ŸåŒ¹é…ï¼ˆä¸Šä¾‹ä¸­ç‚º 8883ï¼‰

# å®šç¾©å·¥å…·å‡½æ•¸
def get_weather(location: str, unit: str = "celsius"):
    """æ¨¡æ“¬çš„å¤©æ°£å‡½æ•¸ç”¨æ–¼æ¸¬è©¦"""
    weather_data = {
        "San Francisco": {"celsius": "18Â°C", "fahrenheit": "64Â°F", "condition": "å¤šéœ§"},
        "New York": {"celsius": "22Â°C", "fahrenheit": "72Â°F", "condition": "æ™´æœ—"},
        "Tokyo": {"celsius": "25Â°C", "fahrenheit": "77Â°F", "condition": "å¤šé›²"},
        "London": {"celsius": "16Â°C", "fahrenheit": "61Â°F", "condition": "ä¸‹é›¨"},
    }
    
    # æœªçŸ¥ä½ç½®çš„é»˜èªå›æ‡‰
    if location not in weather_data:
        return f"{location} çš„å¤©æ°£æ•¸æ“šï¼ˆ{unit}ï¼‰ï¼š20Â°C/68Â°Fï¼Œæ™´æœ—"
    
    data = weather_data[location]
    temp = data[unit]
    condition = data["condition"]
    
    return f"{location} çš„å¤©æ°£æ•¸æ“šï¼š{temp}ï¼Œ{condition}"

# å°‡å‡½æ•¸åæ˜ å°„åˆ°å‡½æ•¸
tool_functions = {
    "get_weather": get_weather,
    "calculator": calculator  # è¨ˆç®—å™¨å‡½æ•¸å¯¦ç¾çœç•¥ä»¥ç°¡åŒ–
}

# ä»¥ OpenAI æ ¼å¼å®šç¾©å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "ç²å–æŒ‡å®šä½ç½®çš„ç•¶å‰å¤©æ°£",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "åŸå¸‚åç¨±ï¼Œä¾‹å¦‚ 'San Francisco'"},
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"], "description": "æº«åº¦å–®ä½"}
                },
                "required": ["location"]
            }
        }
    }
    # è¨ˆç®—å™¨å·¥å…·å®šç¾©çœç•¥ä»¥ç°¡åŒ–
]

# èª¿ç”¨ API ä¸¦è™•ç†å·¥å…·èª¿ç”¨çš„ç¤ºä¾‹
def test_function_calling(question):
    # å®šç¾© FoxBrain ç³»çµ±æç¤ºè©ï¼ˆå¸¶çµæ§‹åŒ–è¼¸å‡ºæŒ‡ç¤ºï¼‰
    system_prompt = """æ‚¨æ˜¯ç”±é´»æµ·ç ”ç©¶é™¢å‰µå»ºå’Œé–‹ç™¼çš„ FoxBrain AI åŠ©æ‰‹...."""
    
    # ä½¿ç”¨å·¥å…·ç™¼é€å®Œæˆè«‹æ±‚
    response = client.chat.completions.create(
        model="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘",  # æ‚¨çš„æ¨¡å‹è·¯å¾‘
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        tools=tools,
        tool_choice="auto",
        temperature=0.5,
        max_tokens=2048,
        stop=['<|eot_id|>', '<|end_of_text|>', '<|end_header_id|>'],
        seed=42,
    )
    
    # ç²å–åŠ©æ‰‹çš„æ¶ˆæ¯
    assistant_message = response.choices[0].message
    
    # è™•ç†å­˜åœ¨çš„å·¥å…·èª¿ç”¨
    if hasattr(assistant_message, 'tool_calls') and assistant_message.tool_calls:
        for tool_call in assistant_message.tool_calls:
            function_call = tool_call.function
            
            # è§£æåƒæ•¸ä¸¦èª¿ç”¨å‡½æ•¸
            arguments = json.loads(function_call.arguments)
            function = tool_functions[function_call.name]
            result = function(**arguments)
            
            # å°‡å‡½æ•¸çµæœç™¼é€å›æ¨¡å‹
            follow_up_response = client.chat.completions.create(
                model="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘",  # æ‚¨çš„æ¨¡å‹è·¯å¾‘
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question},
                    assistant_message,
                    {"role": "tool", "tool_call_id": tool_call.id, "name": function_call.name, "content": result}
                ],
                temperature=0.5,
                max_tokens=2048,
                stop=['<|eot_id|>', '<|end_of_text|>', '<|end_header_id|>'],
                seed=42,
            )
            
            # ç²å–ç´å…¥å·¥å…·çµæœå¾Œçš„æœ€çµ‚å›æ‡‰
            final_response = follow_up_response.choices[0].message.content
```

## å‡½æ•¸èª¿ç”¨å®¢æˆ¶ç«¯å¯¦ç¾

```python
# å®šç¾©å¯ç”¨å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "ç²å–æŒ‡å®šä½ç½®çš„ç•¶å‰å¤©æ°£",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "åŸå¸‚åç¨±"},
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
                },
                "required": ["location"]
            }
        }
    }
]

# å¸¶å·¥å…·çš„è«‹æ±‚
response = client.chat.completions.create(
    model="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "å°åŒ—çš„å¤©æ°£å¦‚ä½•ï¼Ÿ"}
    ],
    tools=tools,
    tool_choice="auto",
    temperature=0.7,
    max_tokens=2048,
)

# è™•ç†å·¥å…·èª¿ç”¨
assistant_message = response.choices[0].message

if hasattr(assistant_message, 'tool_calls') and assistant_message.tool_calls:
    # è™•ç†å·¥å…·èª¿ç”¨
    for tool_call in assistant_message.tool_calls:
        function_call = tool_call.function
        
        # èª¿ç”¨æ‚¨çš„å¯¦éš›å‡½æ•¸ä¸¦ç²å–çµæœ
        result = "å°åŒ—å¤©æ°£ï¼š25Â°Cï¼Œå¤šé›²"  # ç¤ºä¾‹çµæœ
        
        # å°‡çµæœç™¼é€å›æ¨¡å‹
        follow_up = client.chat.completions.create(
            model="FoxBrain æ¨¡å‹æª¢æŸ¥é»çš„è·¯å¾‘",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "å°åŒ—çš„å¤©æ°£å¦‚ä½•ï¼Ÿ"},
                assistant_message,
                {"role": "tool", "tool_call_id": tool_call.id, "name": function_call.name, "content": result}
            ],
            temperature=0.7,
            max_tokens=2048,
        )
        
        # ç²å–æœ€çµ‚å›æ‡‰
        final_response = follow_up.choices[0].message.content
```

---

# è§£æ FoxBrain çµæ§‹åŒ–è¼¸å‡º

FoxBrain æ¨¡å‹å·²ç¶“ç¶“éå¾®èª¿ï¼Œèƒ½å¤ ç”Ÿæˆå¸¶æœ‰ç‰¹å®šæ¨™ç±¤çš„çµæ§‹åŒ–è¼¸å‡ºï¼Œå¦‚ `<step>`ã€`<reflection>` å’Œ `<answer>`ã€‚åœ¨ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œæ‚¨ç¶“å¸¸éœ€è¦æå–é€™äº›çµ„ä»¶é€²è¡Œé€²ä¸€æ­¥è™•ç†ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨æ–¼ FoxBrain è¼¸å‡ºçš„å¼·å¤§è§£æå‡½æ•¸ï¼š

```python
def parse_response(response):
    """è§£æ FoxBrain æ¨¡å‹å›æ‡‰ä¸­çš„çµæ§‹åŒ–å…ƒç´ ã€‚"""
    patterns = {
        "budget": r'<count>([^<]+)</count>',
        "steps": r'<step>([^<]+)</step>',
        "answers": r'<answer>([^<]+)</answer>',
        "reflections": r'<reflection>([^<]+)</reflection>',
        "clarifications": r'<clarification>([^<]+)</clarification>',
        "quality": r'<reward>([^<]+)</reward>'
    }
    
    parsed = {}
    for key, pattern in patterns.items():
        matches = re.findall(pattern, response, re.DOTALL)
        if matches:
            parsed[key] = matches
    
    return parsed

def display_parsed_response(parsed, title="è§£æå¾Œçš„å›æ‡‰"):
    """ä»¥çµæ§‹åŒ–æ ¼å¼é¡¯ç¤ºè§£æå¾Œçš„çµ„ä»¶ã€‚"""
    print(f"\n=== {title} ===")
    
    if not parsed:
        print("å›æ‡‰ä¸­æœªæ‰¾åˆ°çµæ§‹åŒ–å…ƒç´ ã€‚")
        return
    
    if "budget" in parsed:
        print(f"ğŸ”¢ æ­¥é©Ÿé ç®—: {parsed['budget'][0].strip()}")
    
    if "steps" in parsed:
        print("\nğŸ“‹ æ­¥é©Ÿ:")
        for i, step in enumerate(parsed["steps"], 1):
            print(f"  {i}. {step.strip()}")
    
    if "answers" in parsed:
        print("\nâœ… ç­”æ¡ˆ:")
        print(f"  {parsed['answers'][0].strip()}")
    
    if "reflections" in parsed:
        print("\nğŸ¤” åæ€:")
        print(f"  {parsed['reflections'][0].strip()}")
    
    if "quality" in parsed:
        print("\nâ­ è³ªé‡è©•åˆ†:")
        print(f"  {parsed['quality'][0].strip()}")
```

### ä½¿ç”¨ç¯„ä¾‹:

```python
# å¾æ¨¡å‹æ¥æ”¶å›æ‡‰å¾Œ
response_content = assistant_message.content
parsed_components = parse_response(response_content)
display_parsed_response(parsed_components)

# å°æ–¼å‡½æ•¸èª¿ç”¨å›æ‡‰
if has_tool_calls:
    # æå–ç­”æ¡ˆçµ„ä»¶ä»¥é€²è¡Œæ›´æ¸…æ™°çš„å·¥å…·è™•ç†
    answers = parsed_components.get("answers", [])
    if answers:
        print(f"ç”¨æ–¼å·¥å…·è™•ç†çš„æå–ç­”æ¡ˆ: {answers[0]}")
```

é€™ç¨®è§£ææ–¹æ³•åœ¨çµåˆ FoxBrain çš„çµæ§‹åŒ–è¼¸å‡ºåŠŸèƒ½èˆ‡å‡½æ•¸èª¿ç”¨æ™‚ç‰¹åˆ¥æœ‰ç”¨ï¼Œå› ç‚ºå®ƒå…è¨±æ‚¨ï¼š

1. å„ªå…ˆè™•ç†æœ€çµ‚ `<answer>` ç­‰ç‰¹å®šçµ„ä»¶ä»¥é€²è¡Œæ±ºç­–
2. è·Ÿè¸ª FoxBrain åœ¨è¤‡é›œæ¨ç†éç¨‹ä¸­å¦‚ä½•ç®¡ç†å…¶æ­¥é©Ÿé ç®—
3. æå–è§£é‡‹æ¨¡å‹è§£æ±ºå•é¡Œæ–¹æ³•çš„åæ€
4. éš”é›¢çµ„ä»¶ä»¥æ»¿è¶³ä¸åŒçš„ä¸‹æ¸¸è™•ç†éœ€æ±‚

---

## æ€§èƒ½å„ªåŒ–

å°æ–¼ VLLM çš„æœ€ä½³æ€§èƒ½ï¼Œæ ¹æ“šæ‚¨çš„éƒ¨ç½²æ–¹æ³•è€ƒæ…®ä»¥ä¸‹æç¤ºï¼š

## æ€§èƒ½æç¤º

1. **GPU å…§å­˜**ï¼šå°æ–¼åƒ FoxBrain é€™æ¨£çš„ 70B åƒæ•¸æ¨¡å‹ï¼Œæ‚¨éœ€è¦è‡³å°‘ 80GB çš„ GPU å…§å­˜ç”¨æ–¼ FP8 ç²¾åº¦ï¼Œæˆ–è€… 2x40GB GPU ç”¨æ–¼ BF16 ç²¾åº¦ã€‚

2. **å¼µé‡ä¸¦è¡Œ**ï¼šä½¿ç”¨ `--tensor-parallel-size` åœ¨å¤šå€‹ GPU ä¸Šåˆ†ä½ˆæ¨¡å‹ã€‚é€™æ¸›å°‘äº†æ¯ GPU çš„å…§å­˜éœ€æ±‚ï¼Œä½†å¯èƒ½ç•¥å¾®å½±éŸ¿ååé‡ã€‚

3. **é‡åŒ–**ï¼šå°æ–¼å…§å­˜å—é™çš„ç’°å¢ƒï¼Œè€ƒæ…®é‡åŒ–ï¼š
   ```bash
   python -m vllm.entrypoints.openai.api_server \
       --model "FoxBrain æ¨¡å‹çš„è·¯å¾‘" \
       --quantization awq \
       --dtype bfloat16
   ```

4. **æ‰¹é‡å¤§å°**ï¼šèª¿æ•´ `--max-model-len`ï¼ˆé»˜èªç‚º 16384ï¼Œå¯è¨­ç½®ç‚ºæœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦ 128kï¼‰åƒæ•¸æ§åˆ¶ä¸Šä¸‹æ–‡é•·åº¦ï¼Œä¸¦ä½¿ç”¨ `--gpu-memory-utilization`ï¼ˆé»˜èªç‚º 0.97ï¼‰æ§åˆ¶å…§å­˜ä½¿ç”¨ã€‚

5. **é€£çºŒæ‰¹è™•ç†**ï¼šVLLM é»˜èªä½¿ç”¨é€£çºŒæ‰¹è™•ç†ï¼Œé€™æ¯”å‚³çµ±æ‰¹è™•ç†æ›´æœ‰æ•ˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `--max-num-batched-tokens` èª¿æ•´æœ€å¤§æ‰¹å¤§å°ã€‚

---

## å¸¸è¦‹å•é¡Œæ’è§£

å¸¸è¦‹å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆï¼š

1. **å…§å­˜ä¸è¶³**ï¼šæ¸›å°‘å¼µé‡ä¸¦è¡Œå¤§å°ï¼Œä½¿ç”¨é‡åŒ–ï¼Œæˆ–å˜—è©¦è¼ƒå°çš„æ¨¡å‹ã€‚

2. **æ¨ç†ç·©æ…¢**ï¼šæª¢æŸ¥ GPU åˆ©ç”¨ç‡ï¼Œç¢ºä¿æœ‰è¶³å¤ çš„ CPU æ ¸å¿ƒé€²è¡Œé è™•ç†ï¼Œä¸¦è€ƒæ…®ä½¿ç”¨æ›´é«˜çš„é‡åŒ–ç²¾åº¦ã€‚

3. **å‡½æ•¸èª¿ç”¨ä¸­çš„ç„¡æ•ˆ JSON**ï¼šé€™å¯èƒ½ç™¼ç”Ÿåœ¨è¤‡é›œæç¤ºä¸­ã€‚åœ¨æ‡‰ç”¨ç¨‹åºä¸­æ·»åŠ è§£æå’ŒéŒ¯èª¤è™•ç†ï¼Œä»¥è™•ç†ä¸¦ç³¾æ­£æ ¼å¼éŒ¯èª¤çš„ JSONã€‚

4. **ä¸Šä¸‹æ–‡é•·åº¦å•é¡Œ**ï¼šå¦‚æœæ‚¨çœ‹åˆ°è¼¸å‡ºè¢«æˆªæ–·ï¼Œåœ¨å•Ÿå‹•ä¼ºæœå™¨æ™‚å¢åŠ  `--max-model-len` åƒæ•¸ã€‚

æ›´å¤šä¿¡æ¯ï¼Œè«‹åƒè€ƒ [VLLM æ–‡æª”](https://vllm.readthedocs.io/)ã€‚