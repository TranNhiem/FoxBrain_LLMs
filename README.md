# FoxBrain_LLMs
é …ç›®å…§å®¹åŒ…æ‹¬ï¼š1.  å‰µå»ºå¤šæ¨£åŒ–ã€é«˜è³ªé‡çš„ä¸­æ–‡æ•™å­¸æ•¸æ“šé›†ã€‚2. åœ¨é–‹æºèªè¨€æ¨¡å‹ï¼ˆå¦‚bloomzã€LLaMA2ã€Yi, Qwen\ç­‰ï¼‰ä¸Šé€²è¡ŒLLMè¨“ç·´ã€å¾®èª¿ã€è©•ä¼°å’Œæ¸¬è©¦ã€‚Building a diverse and high-quality Chinese instruction dataset. 2. LLM training, finetuning, evaluating, and testing on open-source language models


<h1 align="center">
  <span> FoxBrain - Advancing Language Models Community in Traditional Chinese Roadmap</span>
</h1>

<div align="center">
     <img width="auto" height="400px" src="./Images/Foxbrain_roadmap.png"/>
</div>


## ğŸ’¡ Get help - [Q&A](https://github.com/TranNhiem/FoxBrain_LLMs/discussions) or [Discord ğŸ’¬](https://discord.gg/z7epQGBR7q)

# News: 
+ [2023.08.27] We release BLOOMZ 3B, 7B instruction fine-tuning on 52k Traditional Chinese alpacağŸ”¥
+ [2023.09.02] We release LLaMA2 7B, 13B (4k and 8K Context Length) fine-tuning on 200k Zh_Chinese and English pair Mix Instruction ğŸ”¥

+ [Comming_soon] We release Yi 6B, 34B fine-tuning on 200k Zh_Chinese and English pair Mix Instruction ğŸ”¥


We provide a number of model checkpoints that we trained. Please find them on Hugging Face [here](https://huggingface.co/trannhiem). Here are some quick links to the checkpoints that are finetuned from LLaMa 2:

| **Model**         |                   **Link**                                                            | 
|--------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **FoxBrain v1.0 13B SFT (LLama2 based)**  | ğŸ¤— <a href="" target="_blank">Zh_LLama2_13B_8K_SFT_General_Domain_Knowledge</a>  | 
| **FoxBrain v1.0 7B SFT (LLama2 based)**  | ğŸ¤— <a href="" target="_blank">Zh_llama2_7B_8K_SFT_General_domain</a>  | 

| **FoxBrain v1.0 13 B SFT (LLama2 based)**  | ğŸ¤— <a href="" target="_blank">Zh_LLama2_13B_4K_SFT_General_Domain_Knowledge</a>  | 
| **FoxBrain v1.0 7B SFT (LLama2 based)**  | ğŸ¤— <a href="" target="_blank">Zh_llama2_7B_4K_SFT_General_domain</a>  | 


| **FoxBrain v1.0 SFT 3B (Bloomz Based)** | ğŸ¤— <a href="" target="_blank">Zh_Bloomz_3B_SFT </a>  | 
| **FoxBrain v1.0 SFT 7B (Bloomz Based)** | ğŸ¤— <a href="" target="_blank">Zh_Bloomz_7B_SFT </a>  | 
## Data

Here are some quick links to the datasets that we used to train the models:
| **Dataset**                      | **Link**                                                                                                                        | **Note**                    |
|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|
| **Mix Instruction-tuning**  |  [ Zh Mix Instructions]()                                                                         |                              |
| **Traditional Chinese 52K Alpaca**        | [Zh Alpaca 52k]() | Translated using GPT-3.5    |
| **Traditional Chinese Lima 1K**           | [Zh Lima 1K]()  | Translated by GPT-4         |
| **Zh_Dolly**             | [Traditional Chinese Dolly]()                                                                                  | Translated by GPT-4         |
| **Traditional Chinese Instruction of How**             | [Zh Instruction How Step by Step]()                                                                                  | Extracted from Vietnamese WikiHow       |


# Demo: 

+ [**FoxBrain 13B (Based LLama2 Model) Demo**]()
+ [**FoxBrain 7B (Based LLama2 Model) Demo**]()



<div align="center">
     <img width="auto" height="500px" src="./images/Vietassistant_GPT.gif"/>
</div>


# Table of Contents

- [Project Introduction](#Project-Introduction)
- [Project Goal](#Project-Goals)
- [Project Structure](#Project-Structure)
- [Project Plan](#Project-Plan)
- [How Can You help](#How-can-you-help)

## Project Introduction:

Hello and welcome to the FoxBrain project! This project aims to create Traditional instruction datasets and perform Supervised instruction fine-tuning, as well as Human Preference alignment on various open-source language models such as BloomZ, LLaMa 2, Yi, Qwen, and many others.

## Project Goals:

- Build a high-quality Traditional Chinese Instruction Dataset
- Train, Fine-tune, and Evaluate Multilingual Language Models with a special focus on (Traditional Chinese and English) (Training, Finetuning, Evaluation)
- Design an Application with an optimized User Interface for performance

## Project Structure

DÆ°á»›i Ä‘Ã¢y lÃ  cáº¥u trÃºc cá»§a dá»± Ã¡n, mÃ´ táº£ cÃ¡c pháº§n quan trá»ng vÃ  chá»©c nÄƒng chÃ­nh cá»§a chÃºng:

### benchmark

### 2. Training & Fine-tune LLM Model

<!-- ### 3. Giao Diá»‡n Web (Web UI Interface)

ThÆ° má»¥c `/WebUI` chá»©a cÃ¡c tá»‡p tin vÃ  cÃ´ng cá»¥ liÃªn quan Ä‘áº¿n giao diá»‡n ngÆ°á»i dÃ¹ng qua Web.

- Hiá»‡n táº¡i, Ä‘á»ƒ nhanh chÃ³ng vÃ  thuáº­n tiá»‡n cho viá»‡c demo vÃ  kiá»ƒm thá»­, chÃºng tÃ´i sá»­ dá»¥ng Gradio Ä‘á»ƒ phÃ¡t triá»ƒn giao diá»‡n.

  - `assistant_gradio.py`: ÄÃ¢y lÃ  á»©ng dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn Gradio, cho phÃ©p tráº£i nghiá»‡m trá»±c quan vÃ  trÃ² chuyá»‡n vá»›i trá»£ lÃ½ thÃ´ng qua giao diá»‡n Web.

Hy vá»ng Vá»›i cáº¥u trÃºc nÃ y, dá»± Ã¡n cÃ³ thá»ƒ Ä‘Æ°á»£c quáº£n lÃ½ má»™t cÃ¡ch cá»¥ thá»ƒ vÃ  dá»… Ä‘Ã ng Ä‘á»ƒ cáº­p nháº­p [má»i ngÆ°á»i cÃ³ thá»ƒ gÃ³p Ã½ Ä‘á»ƒ cÃ³ má»™t cáº¥u trÃºc tá»‘t hÆ¡n]() -->


## Project plan

[Project Slide Structure]() 

### BÆ°á»›c 1: Dá»‹ch táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n
- Má»¥c tiÃªu: Dá»‹ch cÃ¡c bá»™ dá»¯ liá»‡u chuáº©n vÃ  cháº¥t LÆ°á»£ng English based instructions dataset : [Alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json), [Dolly 15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1), [Filtered_ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) others dataset.
- XÃ¢y dá»±ng há»‡ thá»‘ng, thá»‘ng kÃª hiá»ƒn thá»‹ cÃ¡c chá»§ Ä‘á» khÃ¡c nhau trong táº­p dá»¯ liá»‡u Ä‘Ã£ thu tháº­p. Má»¥c Ä‘Ã­ch lÃ  loáº¡i bá» dá»¯ liá»‡u chá»©a thÃ´ng tin gÃ¢y láº·n, Ä‘á»™c háº¡i, spam, rÃ¡c rÆ°á»Ÿi hoáº·c thÃ´ng tin cÃ¡ nhÃ¢n hoáº·c cÃ¡c dá»¯ khÃ´ng Ä‘áº¡t yÃªu cáº§u.

### BÆ°á»›c 2: Táº¡o táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tá»± Ä‘á»™ng
- Sá»­ dá»¥ng OpenAI GPT-3.5, GPT-4 Ä‘á»ƒ táº¡o táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n.
- Má»¥c tiÃªu: Thu tháº­p 500.000 Ä‘áº¿n 1 triá»‡u máº«u hÆ°á»›ng dáº«n Ä‘áº§u vÃ o + pháº£n há»“i (Instructions, outputs)
- Äá»“ng thá»i, chÃºng tÃ´i thu tháº­p cÃ¡c hÆ°á»›ng dáº«n Ä‘Æ°á»£c táº¡o bá»Ÿi con ngÆ°á»i cÃ³ sáºµn báº±ng tiáº¿ng Viá»‡t.


ThÆ° má»¥c `/Generate_and_Translate_Dataset` chá»©a cÃ¡c bá»™ dá»¯ liá»‡u vÃ  cÃ´ng cá»¥ liÃªn quan Ä‘áº¿n viá»‡c táº¡o vÃ  dá»‹ch cÃ¡c instruction dataset.

- Pháº§n Dá»‹ch (Translation Dataset)

  - `Using_OpenAI_Translate_API.py`: Sá»­ dá»¥ng OpenAI GPT-3.5 vÃ  GPT-4 Ä‘á»ƒ dá»‹ch cÃ¡c bá»™ dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p cho káº¿t quáº£ tá»‘t.

  - `Using_NLLB_MetaAI_Translate.py`: Sá»­ dá»¥ng NLLB lÃ m mÃ´ hÃ¬nh cho viá»‡c dá»‹ch. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng 54B model Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng Ä‘á»‘i.

- Pháº§n Táº¡o Instruction Dataset 

  - Chi tiáº¿t ká»¹ thuáº­t dÃ¹ng [táº¡o Instruction dataset]() 

  - `Generation_instruction_OpenAI_api.py`: Sá»­ dá»¥ng Stanford Alpaca template Ä‘á»ƒ táº¡o cÃ¡c instruction dataset. Gá»“m hÆ¡n 175 instruction tasks Ä‘Æ°á»£c táº¡o bá»Ÿi con ngÆ°á»i.

  - Using Evolutional algorithm to Generate Instruction Dataset [evol_instruct_generate]()


### BÆ°á»›c 3: Kiá»ƒm Ä‘á»‹nh vÃ  tiá»n xá»­ lÃ½ táº­p dá»¯ liá»‡u
- Káº¿t há»£p táº­p dá»¯ liá»‡u tá»« BÆ°á»›c 1 vÃ  BÆ°á»›c 2.
- Tiá»n xá»­ lÃ½ táº­p dá»¯ liá»‡u Ä‘á»ƒ chuáº©n bá»‹ cho cÃ¡c bÆ°á»›c tiáº¿p theo.

### BÆ°á»›c 4: Tiáº¿n hÃ nh giai Ä‘oáº¡n SFT (Supervised instruction Finetuning)
- Dá»±a trÃªn táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tiáº¿ng Viá»‡t, tiáº¿n hÃ nh giai Ä‘oáº¡n SFT Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh.


ThÆ° má»¥c `/LLMs` chá»©a cÃ¡c tá»‡p tin vÃ  cÃ´ng cá»¥ Ä‘á»ƒ training vÃ  fine-tune cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ (Language Models).

- Pháº§n Fine-tuning dá»±a trÃªn cÃ¡c Open-Source Based LLMs (BLOOMZ, Open-LLaMA, v.v.)

  - `Full Finetune_llm`: fine-tune all layers cÃ¡c mÃ´ hÃ¬nh LLMs dá»±a trÃªn cÃ¡c mÃ£ nguá»“n má»Ÿ nhÆ° BLOOMZ, Open-LLaMA, v.v.

  - `Finetune_llm_QLoRA.py`: Efficient fine-tune cÃ¡c mÃ´ hÃ¬nh LLMs dá»±a trÃªn cÃ¡c mÃ£ nguá»“n má»Ÿ.

### BÆ°á»›c 5: Tiáº¿p tá»¥c huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i giai Ä‘oáº¡n Human Preference RLHF (Reinforcement Learning from Human Feedback) and DPO (Direct Policy Optimization)
- Sau khi hoÃ n thÃ nh BÆ°á»›c 4, chÃºng ta cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i giai Ä‘oáº¡n RLHF dá»±a trÃªn táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tá»« con ngÆ°á»i thuá»™c dá»± Ã¡n OpenAssistant cÃ´ng khai.

HÃ£y nhá»› ráº±ng cÃ¡c bÆ°á»›c nÃ y Ä‘áº¡i diá»‡n cho quy trÃ¬nh chung vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vÃ  bá»• sung theo yÃªu cáº§u cá»¥ thá»ƒ cá»§a dá»± Ã¡n.

## How can you help

+ Would you like to contribute to the project? Please refer to the Contribute_guideline for instructions on how to get started and collaborate on this project together.

```
@misc{vietnameseLLM,
    author={HHRAI},
    title={FoxBrain Instruction Data Corpus for Large-Scale Finetuning of Language Models},
    year={2023},
    url={https://github.com/TranNhiem/FoxBrain_LLMs},
}
```


